{
  "host": "ec2-3-143-6-83.us-east-2.compute.amazonaws.com",
  "scan_id": "scan_1750731280",
  "timestamp": "2025-06-23T22:14:40.517385",
  "scan_duration": 116.5758204460144,
  "processes": [
    {
      "pid": 513,
      "name": "@dbus-daemon",
      "user": "message+",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "@dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation --syslog-only",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 520,
      "name": "/usr/bin/python3",
      "user": "root",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "application",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": [
        "Running as root - consider principle of least privilege"
      ]
    },
    {
      "pid": 623,
      "name": "/usr/sbin/chronyd",
      "user": "_chrony",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/sbin/chronyd -F 1",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 633,
      "name": "/usr/sbin/chronyd",
      "user": "_chrony",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/sbin/chronyd -F 1",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 700,
      "name": "/usr/bin/python3",
      "user": "root",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "application",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": [
        "Running as root - consider principle of least privilege"
      ]
    },
    {
      "pid": 199687,
      "name": "/usr/lib/polkit-1/polkitd",
      "user": "polkitd",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/lib/polkit-1/polkitd --no-debug",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 199688,
      "name": "nginx:",
      "user": "root",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "nginx: master process /usr/sbin/nginx -g daemon on; master_process on;",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "web_server",
      "service_purpose": "Web server and reverse proxy",
      "integrations_detected": [],
      "security_concerns": [
        "Running as root - consider principle of least privilege",
        "Review web server configuration and SSL/TLS settings"
      ]
    },
    {
      "pid": 199689,
      "name": "nginx:",
      "user": "www-data",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "nginx: worker process",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "background_worker",
      "service_purpose": "Background task processing",
      "integrations_detected": [],
      "security_concerns": [
        "Review web server configuration and SSL/TLS settings"
      ]
    },
    {
      "pid": 199710,
      "name": "/usr/sbin/rsyslogd",
      "user": "syslog",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/usr/sbin/rsyslogd -n -iNONE",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 241400,
      "name": "/opt/learnchain/venv/bin/python",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/opt/learnchain/venv/bin/python /opt/learnchain/worker.py",
      "full_command_line": null,
      "working_dir": "/opt/learnchain",
      "service_classification": "background_worker",
      "service_purpose": "Background task processing",
      "integrations_detected": [],
      "security_concerns": [
        "Review worker process security and input validation"
      ]
    },
    {
      "pid": 241401,
      "name": "/opt/learnchain/venv/bin/python",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "/opt/learnchain/venv/bin/python /opt/learnchain/worker-2.py",
      "full_command_line": null,
      "working_dir": "/opt/learnchain",
      "service_classification": "background_worker",
      "service_purpose": "Background task processing",
      "integrations_detected": [],
      "security_concerns": [
        "Review worker process security and input validation"
      ]
    },
    {
      "pid": 274523,
      "name": "(sd-pam)",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "(sd-pam)",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 274631,
      "name": "sshd:",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "sshd: ubuntu@notty",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 275045,
      "name": "sshd:",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "sshd: ubuntu@notty",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    },
    {
      "pid": 275046,
      "name": "ps",
      "user": "ubuntu",
      "cpu_percent": "0",
      "memory_percent": "0",
      "command": "ps aux --no-headers",
      "full_command_line": null,
      "working_dir": null,
      "service_classification": "service",
      "service_purpose": "Application service",
      "integrations_detected": [],
      "security_concerns": []
    }
  ],
  "application_files": [
    {
      "path": "/opt/learnchain/worker.py",
      "language": "Python",
      "size": 4552,
      "last_modified": "2025-05-04T16:08:20",
      "imports": [
        "json",
        "os",
        "pathlib",
        "subprocess",
        "tempfile",
        "time",
        "traceback",
        "boto3",
        "urllib.parse.unquote_plus"
      ],
      "functions": [
        "_delete_msg: Deletes a message from the SQS queue after processing.",
        "handle: Processes a single SQS message, handling document parsing and uploading."
      ],
      "classes": [],
      "external_services": [
        "AWS SQS for message queueing",
        "AWS S3 for file storage and retrieval"
      ],
      "api_endpoints": [],
      "database_models": [],
      "environment_variables": [],
      "security_concerns": [
        {
          "concern_type": "data_exposure",
          "severity": "medium",
          "description": "Sensitive data could be exposed if S3 bucket permissions are not properly configured.",
          "recommendation": "Ensure S3 bucket policies are correctly set to restrict access to authorized users only."
        }
      ],
      "business_logic_summary": "This file implements a worker that listens to an AWS SQS queue for S3 events related to document uploads. It processes these events by downloading the specified PDF files, parsing them using an external script, and uploading the parsed results back to S3. This automation supports the business need for efficient document processing and storage management.",
      "performance_notes": [
        "The use of a single-threaded loop to process messages could become a bottleneck if the queue receives a high volume of messages.",
        "The subprocess call to the parser could be a performance bottleneck if the parsing script is slow or resource-intensive."
      ],
      "complexity_score": 6,
      "documentation_quality": "Good",
      "external_apis": [
        {
          "service": "AWS SQS",
          "endpoint": "https://sqs.us-east-2.amazonaws.com/571600855492/parse-jobs",
          "purpose": "To receive messages about new document uploads that need processing."
        },
        {
          "service": "AWS S3",
          "endpoint": "learnchain-data-prod",
          "purpose": "To download raw documents and upload parsed documents."
        }
      ],
      "database_connections": []
    },
    {
      "path": "/opt/learnchain/worker-2.py",
      "language": "Python",
      "size": 6342,
      "last_modified": "2025-05-23T04:30:08",
      "imports": [
        "json",
        "os",
        "time",
        "traceback",
        "subprocess",
        "boto3",
        "urllib.parse",
        "threading",
        "requests"
      ],
      "functions": [
        "trigger_course_generation_async: Triggers the external course generation service asynchronously.",
        "_delete_msg: Deletes a message from the SQS queue.",
        "handle: Processes an SQS message, uploads a parsing complete flag, triggers course generation, and initiates knowledge graph creation."
      ],
      "classes": [],
      "external_services": [
        "AWS SQS",
        "AWS S3",
        "External Course Generation Service",
        "Neo4j Knowledge Graph Generator"
      ],
      "api_endpoints": [
        {
          "path": "/generate-course",
          "method": "POST",
          "handler_function": "trigger_course_generation_async",
          "parameters": [
            "org_id",
            "user_id",
            "course_id",
            "s3_bucket",
            "provider",
            "model",
            "api_key",
            "max_tokens_per_call",
            "max_tokens"
          ],
          "description": "Triggers the external course generation process using the specified parameters."
        }
      ],
      "database_models": [],
      "environment_variables": [],
      "security_concerns": [
        {
          "concern_type": "data_exposure",
          "severity": "high",
          "description": "The API key for the external course generation service is hardcoded in the script, which poses a risk of exposure.",
          "recommendation": "Store the API key in a secure environment variable or a secrets manager."
        }
      ],
      "business_logic_summary": "This script acts as a worker that listens to an AWS SQS queue for S3 events related to course uploads. It processes events indicating the completion of a course upload, uploads a 'parsing_complete.json' flag to a specified S3 path, triggers an external course generation service, and initiates the creation of a knowledge graph using a Neo4j database. This workflow supports the automation of course content processing and knowledge graph generation, which are critical for the LearnChain platform's functionality.",
      "performance_notes": [
        "The script processes one message at a time, which could be a bottleneck if there are many messages to process.",
        "The visibility timeout for SQS messages is set to 30 minutes, which may need adjustment based on the average processing time."
      ],
      "complexity_score": 7,
      "documentation_quality": "Good",
      "external_apis": [
        {
          "service": "Course Generation Service",
          "endpoint": "http://34.58.125.167:8000/generate-course",
          "purpose": "To trigger the generation of course content using an external service."
        }
      ],
      "database_connections": []
    },
    {
      "path": "/opt/learnchain/parsing_adapter.py",
      "language": "Python",
      "size": 12643,
      "last_modified": "2025-05-13T16:12:27",
      "imports": [
        "argparse",
        "json",
        "os",
        "re",
        "sys",
        "tempfile",
        "collections.Counter",
        "pathlib.Path",
        "typing.Dict",
        "typing.List",
        "dotenv.load_dotenv",
        "llama_cloud_services.LlamaParse",
        "llama_index.core.node_parser.SentenceSplitter",
        "tiktoken.encoding_for_model",
        "hashlib",
        "qdrant_client.QdrantClient",
        "openai.OpenAI",
        "time"
      ],
      "functions": [
        "most_common_block: Identifies the most common header/footer block in document pages.",
        "clean_header_footer: Cleans headers and footers from document pages.",
        "extract_tables: Extracts tables from Markdown pages.",
        "write_file: Writes data to a file, creating directories if necessary.",
        "parse_document: Main function to parse a document, extract data, and save artifacts.",
        "split_markdown: Splits Markdown text into chunks for embedding.",
        "embed_text: Generates embeddings for a given text using OpenAI.",
        "upload_to_qdrant: Uploads a text chunk and its metadata to Qdrant.",
        "parse_metadata_from_prefix: Parses metadata from a given S3 prefix."
      ],
      "classes": [],
      "external_services": [
        "LlamaParse for document parsing",
        "OpenAI for text embeddings",
        "Qdrant for vector storage"
      ],
      "api_endpoints": [],
      "database_models": [],
      "environment_variables": [
        {
          "variable_name": "LLAMA_CLOUD_API_KEY",
          "usage_context": "Used to authenticate requests to the LlamaParse service.",
          "default_value": null,
          "is_required": true,
          "description": "API key for accessing LlamaParse services."
        },
        {
          "variable_name": "DISABLE_IMG",
          "usage_context": "Overrides the --keep-images flag to disable image extraction.",
          "default_value": "true",
          "is_required": false,
          "description": "Flag to disable image extraction."
        }
      ],
      "security_concerns": [
        {
          "concern_type": "data_exposure",
          "severity": "high",
          "description": "Hardcoded API keys for OpenAI and Qdrant are present in the script.",
          "recommendation": "Store API keys in environment variables or a secure vault."
        }
      ],
      "business_logic_summary": "This script processes documents using the LlamaParse service, extracts and cleans text, images, and tables, and stores the processed data in Markdown format. It also embeds text chunks into vectors using OpenAI's API and uploads them to a Qdrant vector database for further analysis or retrieval. This is useful for businesses needing to parse and analyze large volumes of documents, such as educational content, and store them in a structured format for easy access and retrieval.",
      "performance_notes": [
        "The script processes documents and extracts data in a sequential manner, which might be a bottleneck for large documents or high volumes of documents.",
        "Embedding text and uploading to Qdrant could be time-consuming depending on the document size and network latency."
      ],
      "complexity_score": 7,
      "documentation_quality": "Good",
      "external_apis": [
        {
          "service": "LlamaParse",
          "endpoint": "LlamaParse API",
          "purpose": "Used for parsing documents and extracting text, images, and tables."
        },
        {
          "service": "OpenAI",
          "endpoint": "OpenAI Embeddings API",
          "purpose": "Used to generate embeddings for text chunks."
        },
        {
          "service": "Qdrant",
          "endpoint": "Qdrant API",
          "purpose": "Used to store and manage vector embeddings for document chunks."
        }
      ],
      "database_connections": [
        "QdrantClient connection to Qdrant vector database."
      ]
    },
    {
      "path": "/opt/learnchain/create_course_knowledge_graph_neo.py",
      "language": "Python",
      "size": 48421,
      "last_modified": "2025-05-20T16:33:49",
      "imports": [
        "os",
        "json",
        "hashlib",
        "logging",
        "boto3",
        "langchain_core.documents.Document",
        "langchain_openai.ChatOpenAI",
        "langchain_openai.OpenAIEmbeddings",
        "langchain_experimental.graph_transformers.LLMGraphTransformer",
        "langchain_neo4j.Neo4jGraph",
        "time",
        "langchain_core.callbacks.base.BaseCallbackHandler"
      ],
      "functions": [
        "list_all_doc_paths: Lists all document UUIDs from a given course's S3 directory.",
        "get_doc_name: Retrieves the real document name from the S3 path.",
        "get_chunks_for_doc: Retrieves all chunks for a specific document directly without re-chunking."
      ],
      "classes": [
        "LLMCallTracker: Tracks calls to the language model, including prompts and responses."
      ],
      "external_services": [
        "AWS S3 for document storage",
        "OpenAI for language model processing",
        "Neo4j for graph database management"
      ],
      "api_endpoints": [],
      "database_models": [],
      "environment_variables": [],
      "security_concerns": [
        {
          "concern_type": "data_exposure",
          "severity": "critical",
          "description": "Hardcoded AWS and OpenAI credentials are present in the script, which can lead to unauthorized access if exposed.",
          "recommendation": "Use environment variables or a secure vault to manage sensitive credentials."
        }
      ],
      "business_logic_summary": "This script is responsible for creating a knowledge graph for courses using Neo4j and OpenAI's language models. It processes documents stored in an S3 bucket, extracts relevant information, and transforms it into a graph structure for enhanced data insights and relationships.",
      "performance_notes": [
        "The script uses paginated requests to S3, which is efficient for handling large datasets.",
        "The use of multiple prefixes for S3 paths can lead to increased latency if not managed properly."
      ],
      "complexity_score": 7,
      "documentation_quality": "Fair",
      "external_apis": [
        {
          "service": "AWS S3",
          "endpoint": "https://s3.amazonaws.com",
          "purpose": "To store and retrieve course documents and their metadata."
        },
        {
          "service": "OpenAI",
          "endpoint": "https://api.openai.com",
          "purpose": "To generate embeddings and transform documents into a graph structure using language models."
        },
        {
          "service": "Neo4j",
          "endpoint": "neo4j+s://9c20b7c4.databases.neo4j.io",
          "purpose": "To store and query the knowledge graph created from course documents."
        }
      ],
      "database_connections": [
        "Neo4j database connection using URI, username, and password."
      ]
    }
  ],
  "infrastructure_insights": {
    "architecture_pattern": "Unknown",
    "technology_stack": [],
    "deployment_model": "Unknown",
    "scalability_assessment": "Unknown",
    "security_posture": "Unknown",
    "operational_complexity": "Unknown",
    "recommendations": [
      "Several security concerns are noted, including hardcoded API keys and credentials, which pose a risk of unauthorized access. Proper management of sensitive data through environment variables or secure vaults is recommended.",
      "**Complexity**: Moderate",
      "The integration of multiple external services and handling of sensitive data increases operational complexity. However, the use of cloud-native services simplifies some aspects of deployment and scaling.",
      ". **Enhance Security**:",
      "Store all sensitive credentials, such as API keys and AWS credentials, in environment variables or a secure secrets manager to prevent unauthorized access.",
      "Review and tighten S3 bucket policies to ensure data is only accessible to authorized users.",
      ". **Improve Scalability**:",
      "Consider implementing multi-threading or asynchronous processing to handle higher volumes of SQS messages efficiently.",
      "Optimize subprocess calls by evaluating the performance of external scripts and exploring more efficient alternatives if necessary.",
      ". **Optimize Performance**:",
      "Adjust the visibility timeout for SQS messages based on actual processing times to prevent message duplication or loss.",
      "Monitor and optimize network latency and resource usage for operations involving external APIs and databases.",
      "Maintain comprehensive documentation for all integrations and workflows to facilitate easier troubleshooting and onboarding.",
      "Implement robust monitoring and logging to track performance metrics and quickly identify issues in the processing pipeline.",
      "Regularly review and update cloud configurations to align with best practices for security, cost management, and performance optimization.",
      "Consider using AWS CloudFormation or Terraform for infrastructure as code to manage and automate deployments consistently."
    ]
  },
  "security_analysis": {
    "analysis": "Based on the provided context and analysis request, here's a comprehensive security assessment focusing on the key areas of concern:\n\n### 1. Security Vulnerabilities and Risks\n\n- **Hardcoded Credentials**: Multiple scripts contain hardcoded API keys and credentials for services like OpenAI, AWS, and Qdrant. This poses a significant risk of unauthorized access if the code is exposed.\n- **Running Processes as Root**: Several processes, including Python scripts and Nginx, are running with root privileges, which violates the principle of least privilege and increases the risk of system compromise.\n- **S3 Bucket Permissions**: There is a risk of data exposure if S3 bucket permissions are not properly configured to restrict access to authorized users only.\n\n### 2. Access Control and Authentication Mechanisms\n\n- **Lack of Environment Variables**: Sensitive information such as API keys and credentials should be stored in environment variables or a secure secrets manager instead of being hardcoded.\n- **Principle of Least Privilege**: Processes should not run with more privileges than necessary. Consider running services with dedicated, non-root users with minimal permissions.\n\n### 3. Network Security Posture\n\n- **Web Server Configuration**: Nginx configuration should be reviewed to ensure secure SSL/TLS settings and to prevent potential vulnerabilities.\n- **External Service Communication**: Ensure that communication with external services (e.g., OpenAI, Neo4j) is encrypted and authenticated.\n\n### 4. Data Protection Measures\n\n- **S3 Bucket Policies**: Review and tighten S3 bucket policies to ensure data is only accessible to authorized users.\n- **Data Encryption**: Ensure that data at rest and in transit is encrypted using strong encryption standards.\n\n### 5. Code-Level Security Concerns\n\n- **Input Validation**: Ensure that all inputs, especially those from external sources, are properly validated and sanitized to prevent injection attacks.\n- **Error Handling**: Implement robust error handling to prevent information leakage through error messages.\n\n### 6. Priority Security Recommendations\n\n1. **Remove Hardcoded Credentials**: \n   - Store all sensitive credentials in environment variables or a secure secrets manager like AWS Secrets Manager or HashiCorp Vault.\n   - Update scripts to retrieve credentials from these secure locations.\n\n2. **Implement Least Privilege Principle**:\n   - Reconfigure services to run with non-root users with the minimum necessary permissions.\n   - For Nginx, ensure that only the master process runs as root, and worker processes run as a non-privileged user.\n\n3. **Review and Secure S3 Bucket Policies**:\n   - Audit S3 bucket policies to ensure they follow the principle of least privilege.\n   - Use AWS IAM roles and policies to control access to S3 buckets.\n\n4. **Enhance Web Server Security**:\n   - Review Nginx configuration for secure SSL/TLS settings.\n   - Implement HTTP security headers to protect against common web vulnerabilities.\n\n5. **Improve Network Security**:\n   - Ensure all external service communications are encrypted using TLS.\n   - Consider using a VPN or private network for sensitive service communications.\n\n6. **Regular Security Audits and Monitoring**:\n   - Implement continuous monitoring and logging to detect and respond to security incidents promptly.\n   - Regularly review and update cloud configurations to align with security best practices.\n\nBy addressing these vulnerabilities and implementing the recommended security measures, the overall security posture of the infrastructure can be significantly improved.",
    "key_findings": [],
    "recommendations": [
      "**Lack of Environment Variables**: Sensitive information such as API keys and credentials should be stored in environment variables or a secure secrets manager instead of being hardcoded.",
      "**Principle of Least Privilege**: Processes should not run with more privileges than necessary. Consider running services with dedicated, non-root users with minimal permissions.",
      "**Web Server Configuration**: Nginx configuration should be reviewed to ensure secure SSL/TLS settings and to prevent potential vulnerabilities.",
      "**External Service Communication**: Ensure that communication with external services (e.g., OpenAI, Neo4j) is encrypted and authenticated.",
      "**S3 Bucket Policies**: Review and tighten S3 bucket policies to ensure data is only accessible to authorized users.",
      "**Data Encryption**: Ensure that data at rest and in transit is encrypted using strong encryption standards.",
      "**Input Validation**: Ensure that all inputs, especially those from external sources, are properly validated and sanitized to prevent injection attacks.",
      "**Error Handling**: Implement robust error handling to prevent information leakage through error messages.",
      ". **Remove Hardcoded Credentials**:",
      "Store all sensitive credentials in environment variables or a secure secrets manager like AWS Secrets Manager or HashiCorp Vault.",
      "Update scripts to retrieve credentials from these secure locations.",
      ". **Implement Least Privilege Principle**:",
      "Reconfigure services to run with non-root users with the minimum necessary permissions.",
      "For Nginx, ensure that only the master process runs as root, and worker processes run as a non-privileged user.",
      ". **Review and Secure S3 Bucket Policies**:",
      "Audit S3 bucket policies to ensure they follow the principle of least privilege.",
      "Use AWS IAM roles and policies to control access to S3 buckets.",
      "Review Nginx configuration for secure SSL/TLS settings.",
      "Implement HTTP security headers to protect against common web vulnerabilities.",
      "Ensure all external service communications are encrypted using TLS.",
      "Consider using a VPN or private network for sensitive service communications.",
      "Implement continuous monitoring and logging to detect and respond to security incidents promptly.",
      "Regularly review and update cloud configurations to align with security best practices."
    ],
    "insights": []
  },
  "business_intelligence": {
    "analysis": "### Business Intelligence Analysis\n\n#### Business Domain\n- **Domain**: Education\n\n#### Primary Business Functions\n- Automated document processing and storage management.\n- Course content generation and knowledge graph creation.\n- Text parsing and embedding for educational content analysis.\n\n#### Application Purpose\n- **Problem Solved**: The system automates the processing of educational documents and course content, facilitating efficient storage, retrieval, and analysis. It supports the creation of structured knowledge graphs, enhancing data insights and relationships for educational platforms.\n\n#### Data Flows\n1. **From**: AWS S3  \n   **To**: AWS SQS  \n   **Data Type**: Document metadata  \n   **Description**: Triggers events for document uploads that need processing.\n\n2. **From**: AWS S3  \n   **To**: External Course Generation Service  \n   **Data Type**: Course content  \n   **Description**: Initiates course generation after document parsing.\n\n3. **From**: AWS S3  \n   **To**: Neo4j  \n   **Data Type**: Parsed document data  \n   **Description**: Transforms document data into a knowledge graph structure.\n\n4. **From**: Local Parsing Script  \n   **To**: Qdrant  \n   **Data Type**: Text embeddings  \n   **Description**: Stores vector embeddings for document chunks for analysis.\n\n#### Critical Workflows\n- Document parsing and uploading to AWS S3.\n- Course content generation and flagging completion.\n- Knowledge graph creation using Neo4j.\n- Text embedding and storage in Qdrant for retrieval.\n\n#### Integration Architecture\n- **External APIs**: \n  - AWS SQS for message queueing.\n  - AWS S3 for document storage.\n  - External Course Generation Service.\n  - LlamaParse for document parsing.\n  - OpenAI for text embeddings.\n  - Neo4j for graph database management.\n  - Qdrant for vector storage.\n- **Databases**: \n  - Neo4j for knowledge graph storage.\n  - Qdrant for vector embeddings.\n- **Message Queues**: \n  - AWS SQS.\n- **Caching**: \n  - Not explicitly mentioned.\n\n#### Scaling Characteristics\n- **Bottlenecks**:\n  - Single-threaded message processing could slow down with high message volumes.\n  - Subprocess calls for parsing and embedding could be resource-intensive.\n- **Scaling Strategy**: \n  - Both horizontal and vertical scaling may be needed to address bottlenecks.\n- **Resource Intensive Operations**:\n  - Document parsing and embedding.\n  - Knowledge graph creation and management.\n\n#### Technology Insights\n- **Framework Patterns**: \n  - Utilizes AWS services for cloud-based operations.\n  - Integration with external APIs for specialized processing.\n- **Architectural Style**: \n  - Event-driven architecture leveraging AWS SQS and S3.\n- **Deployment Complexity**: \n  - Moderate, due to integration with multiple external services and handling of sensitive data.\n\n### Business Value\nThis system provides significant value to educational platforms by automating the processing and analysis of educational content. It enhances the efficiency of document management and supports advanced data insights through knowledge graph creation. The integration with cloud services and external APIs allows for scalable and flexible operations, crucial for handling large volumes of educational data. The system's ability to transform unstructured data into structured formats and insights is a key differentiator in the education domain.",
    "key_findings": [],
    "recommendations": [],
    "insights": [
      "**Problem Solved**: The system automates the processing of educational documents and course content, facilitating efficient storage, retrieval, and analysis. It supports the creation of structured knowledge graphs, enhancing data insights and relationships for educational platforms.",
      ". **From**: AWS S3",
      "*To**: AWS SQS",
      "*Data Type**: Document metadata",
      "*Description**: Triggers events for document uploads that need processing.",
      ". **From**: AWS S3",
      "*To**: External Course Generation Service",
      "*Data Type**: Course content",
      "*Description**: Initiates course generation after document parsing.",
      ". **From**: AWS S3",
      "*To**: Neo4j",
      "*Data Type**: Parsed document data",
      "*Description**: Transforms document data into a knowledge graph structure.",
      "*To**: Qdrant",
      "*Data Type**: Text embeddings",
      "*Description**: Stores vector embeddings for document chunks for analysis.",
      "Document parsing and uploading to AWS S3.",
      "Course content generation and flagging completion.",
      "Knowledge graph creation using Neo4j.",
      "Text embedding and storage in Qdrant for retrieval.",
      "**External APIs**:",
      "AWS SQS for message queueing.",
      "AWS S3 for document storage.",
      "External Course Generation Service.",
      "LlamaParse for document parsing.",
      "OpenAI for text embeddings.",
      "Neo4j for graph database management.",
      "Qdrant for vector storage.",
      "**Databases**:",
      "Neo4j for knowledge graph storage.",
      "Qdrant for vector embeddings.",
      "**Message Queues**:",
      "AWS SQS.",
      "**Caching**:",
      "Not explicitly mentioned.",
      "**Bottlenecks**:",
      "Single-threaded message processing could slow down with high message volumes.",
      "Subprocess calls for parsing and embedding could be resource-intensive.",
      "**Scaling Strategy**:",
      "Both horizontal and vertical scaling may be needed to address bottlenecks.",
      "**Resource Intensive Operations**:",
      "Document parsing and embedding.",
      "Knowledge graph creation and management.",
      "**Framework Patterns**:",
      "Utilizes AWS services for cloud-based operations.",
      "Integration with external APIs for specialized processing.",
      "**Architectural Style**:",
      "Event-driven architecture leveraging AWS SQS and S3.",
      "**Deployment Complexity**:",
      "Moderate, due to integration with multiple external services and handling of sensitive data."
    ]
  },
  "api_documentation": {
    "endpoints": [
      {
        "path": "/generate-course",
        "method": "POST",
        "handler_function": "trigger_course_generation_async",
        "parameters": [
          "org_id",
          "user_id",
          "course_id",
          "s3_bucket",
          "provider",
          "model",
          "api_key",
          "max_tokens_per_call",
          "max_tokens"
        ],
        "description": "Triggers the external course generation process using the specified parameters."
      }
    ],
    "models": [],
    "authentication": [],
    "base_url": "http://localhost"
  },
  "deployment_intelligence": {
    "deployment_type": "unknown",
    "service_management": "systemd",
    "containerization": false,
    "web_server": "nginx",
    "process_manager": null,
    "environment_setup": [
      {
        "variable_name": "LLAMA_CLOUD_API_KEY",
        "usage_context": "Used to authenticate requests to the LlamaParse service.",
        "default_value": null,
        "is_required": true,
        "description": "API key for accessing LlamaParse services."
      },
      {
        "variable_name": "DISABLE_IMG",
        "usage_context": "Overrides the --keep-images flag to disable image extraction.",
        "default_value": "true",
        "is_required": false,
        "description": "Flag to disable image extraction."
      }
    ]
  },
  "llm_analysis_summary": {
    "total_llm_calls": 9,
    "analysis_stages": 6,
    "overall_confidence": 0.9,
    "performance_metrics": {
      "total_duration": 107.08462595939636,
      "avg_call_duration": 11.898291773266262
    }
  },
  "scan_statistics": {
    "processes_analyzed": 15,
    "files_analyzed": 4,
    "analysis_depth": "intelligent",
    "ai_enabled": true,
    "business_intelligence_enabled": true,
    "total_commands_executed": 52
  },
  "analysis_stages": [
    {
      "stage": "connection",
      "status": "completed",
      "timestamp": "2025-06-23T22:14:40.517385"
    },
    {
      "stage": "discovery",
      "status": "completed",
      "timestamp": "2025-06-23T22:14:40.517385"
    },
    {
      "stage": "analysis",
      "status": "completed",
      "timestamp": "2025-06-23T22:14:40.517385"
    },
    {
      "stage": "business_intelligence",
      "status": "completed",
      "timestamp": "2025-06-23T22:14:40.517385"
    }
  ]
}